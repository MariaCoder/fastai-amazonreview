{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataprep - Electronics and TestData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data here:  http://jmcauley.ucsd.edu/data/amazon/links.html\n",
    "<br>Project Notes: https://drive.google.com/file/d/1gQqMqMfF-JMjW0ihN5n16u6YzxeUzi6v/viewm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About this workbook:\n",
    "\n",
    "The purpose of this workbook is to clean and prepare the data. In summary, we will:\n",
    "0. Convert Datatypes: Our initial aim was to concert to smaller datatypes in order to work more efficiently. \n",
    "1. Remove reviews with duplicate reviewText (keeping only one)\n",
    "2. Remove reviews that have received less than 50 votes regarding their helpfulness\n",
    "3. Assign a value of 1.0 (for helpful) to any review which 75% or more of voters have assessed as \"Helpful\". Any review with less than a 75% rating is assigned a value of 0.0 for \"Not Helpful\"\n",
    "4. Create a balanced dataset: Step 3 will result in unbalanced data, where the majority of all reviews will be rated \"helpful\". To account for such an unbalanced dataset and avoid training the classifier to simply pick the more frequent class, initial experiments will train on a balanced dataset where 50% of the data is \"helpful\" and 50% is \"not helpful\". Since the majority of our data is rated \"helpful\"\n",
    "5. Initial experiments also use the .from_folder method for TextBlock, and therefore require pre-populated \"train\" and \"test\" folders.Populating the folders is done in Workbook electronics_data_prep2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.70409917831421\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "start = time.time()\n",
    "df = pd.read_json('../Review_Data/reviews_Electronics_5.json.gz', lines=True, compression=\"gzip\")\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I like what you guys have done to validate that our \"helpful\" column really does contain a list as expected of length 2. For reference, we expect [a,b] where a is the number of people who voted \"yes, this is a helpful review\"  and b is the number of people who voted in total. a-b are the curmudgeons who didn't like the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"helpful\"].apply(lambda x: len(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df[\"overall\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that most people give fairly high ratings: The 5.0 category has the most values. Let us drop all columns we will not need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"reviewerID\", \"asin\", \"reviewerName\", \"unixReviewTime\", \"reviewTime\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Datatypes\n",
    "As recommended by Gaurav, let's see if we can't cast to smaller datatypes. First, let's see what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is object? Shouldn't the datatype for \"reviewText\" and \"summary\" be a string?\n",
    "Let's check again - maybe it's strings and something else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviewText'].apply(lambda x: type(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like strings, but maybe there's something else hiding? Let's get a summary using value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['reviewText'].apply(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summary'].apply(lambda x: type(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for both columns, there really are just strings but pandas says they're objects. Let's cast them as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'overall':'float32','reviewText':'str', 'summary':'str'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't think there's an \"inplace\" parameter here, so I just assigned it to \"df\" again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh. If I run dtype on that dataframe, those columns still say object, not string. Oh well. A mystery to be left for another day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding columns for helpful votes, total votes, and the ratio\n",
    "We want to make three columns: \"num_helpful_votes\", \"num_total_votes\", \"helpful_ratio\".\n",
    "The first two columns should be ints, but I'm casting them to floats because they're the numerator and denominator for the division that will be saved in the third column. That way I ensure that third column contains floats. \n",
    "I'll just cast the first two back to ints later.\n",
    "\n",
    "I think Dewsey does the same thing in a beautiful squish function. His function saves you from having to ensure that the first two are floats, and then recast as ints, and from having to use fillna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "df[\"num_helpful_votes\"] = df[\"helpful\"].apply(lambda x: float(x[0]))\n",
    "df[\"num_total_votes\"] = df[\"helpful\"].apply(lambda x: float(x[1]))\n",
    "df[\"helpful_ratio\"] = df[\"num_helpful_votes\"] / df[\"num_total_votes\"]\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'num_helpful_votes': 'int32', 'num_total_votes': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace the NaN in helpful_ratio with zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['helpful_ratio'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicates and separating out the LM dataset and the Classification dataset\n",
    "\n",
    "#### Duplicates\n",
    "\n",
    "There are duplicate reviews, likely by scammers. Maybe they can be leveraged somehow later, but for now, we're removing them.\n",
    "\n",
    "#### Language Model Dataset versus Classification Dataset\n",
    "Some reviews have too little \"helpfulness\" votes to be of value for the classification task, but they can be useful for the Language Model fine tuning. \n",
    "1. threshhold for determining whether a review should be included in the classification task\n",
    "The literature used 50. Let's try this. Depending on how much that leaves us, we might reduce it to something smaller.\n",
    "2. LM Dataset: We theoretically could use all reviews (minus the duplicates) to finetune the language model. However, that would be more than 1 million reviews which is too many because:\n",
    "<br>a) The fine-tuning of the imdb language model only needed 100K reviews and got good results\n",
    "<br>b) I tried it twice and the kernel died each time. \n",
    "\n",
    "<br>So in summary, it is not necessary for good results and it is for us not practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq = df.drop_duplicates(subset=\"reviewText\", keep = \"first\") #we could use inplace=True but I want to keep the old data\n",
    "#frame around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original idea was to use as many reviews as possible to finetine the Language Model, and planned to use df_uniq for this since it contained all reviews (minus the duplicates). That turned out to be not feasible (and crashed my kernel several times). However, filtering out all reviews with less 50 helpfulness votes yields us a smaller dataframe of 19K reviews, which proved useful for LM finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter for the rows where the num_total_votes is 50 or more, and see how many there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq[df_uniq['num_total_votes'] > 49 ].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, 19,009. Let's see how far that takes us - let's save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_cl = df_uniq[df_uniq['num_total_votes'] > 49 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_cl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's bin and balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0.0, 0.75, 1.0]\n",
    "df_uniq_cl[\"is_helpful\"] = pd.cut(df_uniq_cl[\"helpful_ratio\"], bins, labels=[\"0\", \"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_cl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_cl[\"is_helpful\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### so we need to redo the below a little differently:\n",
    "I need: \n",
    "1. append the two below so I make dataframe balanced\n",
    "2. balanced dataframe split into test and train\n",
    "3. In another workbook, I can use the \"test\" and \"train\" dataframes to populate the respective test and train folders, which can then be used in training the LM and Classifier (TextBlock.from_folder method for example requires this pre-populated folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sampling rom \"helpful\" in order to achieve an equal number of helpful and unhelpful votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unhelpful = df_uniq_cl.loc[df_uniq_cl[\"is_helpful\"] == \"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unhelpful['is_helpful'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_helpful = df_uniq_cl.loc[df_uniq_cl[\"is_helpful\"] == \"1\"].sample(n=2010, random_state=42, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_helpful[\"is_helpful\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the helpful/unhelpful dataframes look good. Let's append them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df = all_helpful.append(all_unhelpful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_df['is_helpful'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The balanced dataframe also looks good. We have equal counts of \"1\" and \"0\" in the 'is_helpful' column. Now, let's divide this into test and training data. Borrowing' Gaurav's approach here again. Note: I bet there is a splitter method in fastai, but so far, I've seen that splitter method integrated into Data loader classes, so I don't know where to find it alone at the moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(balanced_df, test_size=0.20, stratify=balanced_df[\"is_helpful\"], random_state=42)\n",
    "\n",
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(\"electronics_5_train.csv\", header=True, index=False)\n",
    "test_df.to_csv(\"electronics_5_test.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also export the full helpful/unhelpful ones, but no need.We could also export the entire set of training/test data, uniqued, but not balanced (df_uniq_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_cl.to_csv(\"electronics_all_classifier_data_unbalanced_uniq.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before exporting the LM dataset, let's remove the columns we don't need for training the LM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_LM = df_uniq.drop([\"helpful\",\"overall\",'num_helpful_votes','num_total_votes','helpful_ratio'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_LM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uniq_LM.to_csv(\"electronics_for_LM_uniq.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
